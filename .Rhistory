# training building0 - method: knn
knnTrain_building0 <- train(Floor~.,
data = sample_building0,
method = "knn",
trControl = fitControl,
tuneLength = 2,
preProcess=c("center","scale"))
#training building0 - method: random forest
rfTrain_building0 <- train(Floor~.,
data = sample_building0,
method = "rf",
trControl = fitControl,
preProcess=c("center","scale"))
#training building0 - method: svm linear
svmTrain_building0 <- train(Floor~.,
data = sample_building0,
method = "svmLinear",
trControl = fitControl,
tuneLength = 2,
preProcess=c("center","scale"))
library(tidyverse)
library(lattice)
library(caret)
source("C:/Users/Splendora/OneDrive/Ubiqum/Module_3_Task_3_EvaluateWiFi/M3T3_EvaluateWiFi/M3T3_preprocess_script.R")
# Separate data by building from sample_set
sample_building0 <- filter(sample_set, Building_ID == 0)
sample_building1 <- filter(sample_set, Building_ID == 1)
sample_building2 <- filter(sample_set, Building_ID == 2)
sample_building3 <- filter(sample_set, Building_ID == 3)
# Create a data frame for each feature from sample_set
## Building0
sample_building0_floor <- data.frame(sample_building0$Floor, sample_building0[,1:520])
sample_building0_latitude <- data.frame(sample_building0$Latitude, sample_building0[,1:520])
sample_building0_longitude <- data.frame(sample_building0$Longitude, sample_building0[,1:520])
## Building1
sample_building1_floor <- data.frame(sample_building1$Floor, sample_building1[,1:520])
sample_building1_latitude <- data.frame(sample_building1$Latitude, sample_building1[,1:520])
sample_building1_longitude <- data.frame(sample_building1$Longitude, sample_building1[,1:520])
## Building2
sample_building2_floor <- data.frame(sample_building2$Floor, sample_building2[,1:520])
sample_building2_latitude <- data.frame(sample_building2$Latitude, sample_building2[,1:520])
sample_building2_longitude <- data.frame(sample_building2$Longitude, sample_building2[,1:520])
## Building3
sample_building3_floor <- data.frame(sample_building3$Floor, sample_building3[,1:520])
sample_building3_latitude <- data.frame(sample_building3$Latitude, sample_building3[,1:520])
sample_building3_longitude <- data.frame(sample_building3$Longitude, sample_building3[,1:520])
# applying 10-fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 2)
library(tidyverse)
library(lattice)
library(caret)
# import training data ====
training_orig <- read.csv("C:/Users/Splendora/OneDrive/Ubiqum/Module_3_Task_3_EvaluateWiFi/trainingData.csv", header = TRUE)
test_orig <- read.csv("C:/Users/Splendora/OneDrive/Ubiqum/Module_3_Task_3_EvaluateWiFi/validationData.csv", header = TRUE)
# check attributes
attributes(training_orig)
# summarize data
summary(training_orig$Latitude)
# check data type
str(training_orig)
# check for missing values
sum(is.na(training_orig))
# Rename attribute columns
colnames(training_orig)[521] <- "Longitude"
colnames(training_orig)[522] <- "Latitude"
colnames(training_orig)[523] <- "Floor"
colnames(training_orig)[524] <- "Building_ID"
colnames(training_orig)[525] <- "Space_ID"
colnames(training_orig)[526] <- "Relative_Position"
colnames(training_orig)[527] <- "User_ID"
colnames(training_orig)[528] <- "Phone_ID"
colnames(training_orig)[529] <- "Timestamp"
# Recategorize attributes
training_orig$Building_ID <- as.factor(training_orig$Building_ID)
# Visualize min/max values of non-RSSI attributes
nonRSSI_orig <- training_orig %>%
select(Longitude, Latitude, Floor, Building_ID, Space_ID, Relative_Position, User_ID, Phone_ID, Timestamp)
str(nonRSSI_orig)
summary(nonRSSI_orig)
# Create 10% Sample Set from training_orig data
set.seed(745)
sample_orig <- round(nrow(training_orig)*0.10)
remaining_orig <- nrow(training_orig) - sample_orig
training_indices <- sample(seq_len(nrow(training_orig)), size = sample_orig)
sample_set <- training_orig[training_indices,]
remaining_set <- training_orig[-training_indices,]
# Separate data by building from sample_set
sample_building0 <- filter(sample_set, Building_ID == 0)
sample_building1 <- filter(sample_set, Building_ID == 1)
sample_building2 <- filter(sample_set, Building_ID == 2)
sample_building3 <- filter(sample_set, Building_ID == 3)
# Create a data frame for each feature from sample_set
## Building0
sample_building0_floor <- data.frame(sample_building0$Floor, sample_building0[,1:520])
sample_building0_latitude <- data.frame(sample_building0$Latitude, sample_building0[,1:520])
sample_building0_longitude <- data.frame(sample_building0$Longitude, sample_building0[,1:520])
## Building1
sample_building1_floor <- data.frame(sample_building1$Floor, sample_building1[,1:520])
sample_building1_latitude <- data.frame(sample_building1$Latitude, sample_building1[,1:520])
sample_building1_longitude <- data.frame(sample_building1$Longitude, sample_building1[,1:520])
## Building2
sample_building2_floor <- data.frame(sample_building2$Floor, sample_building2[,1:520])
sample_building2_latitude <- data.frame(sample_building2$Latitude, sample_building2[,1:520])
sample_building2_longitude <- data.frame(sample_building2$Longitude, sample_building2[,1:520])
## Building3
sample_building3_floor <- data.frame(sample_building3$Floor, sample_building3[,1:520])
sample_building3_latitude <- data.frame(sample_building3$Latitude, sample_building3[,1:520])
sample_building3_longitude <- data.frame(sample_building3$Longitude, sample_building3[,1:520])
# applying 10-fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 2)
# training building0 - method: knn
knnTrain_building0 <- train(Floor~.,
data = sample_building0,
method = "knn",
trControl = fitControl,
tuneLength = 2,
preProcess=c("center","scale"))
ggplot(knnTraining_building0)
ggplot(knnTrain_building0)
knnTrain_building0
rfTrain_building0 <- train(Floor~.,
data = sample_building0,
method = "rf",
trControl = fitControl,
preProcess=c("center","scale"))
knnTrain_building0
ggplot(knnTrain_building0)
sample_building0[,starts_with("WAP")]
sample_building0 %>% select(starts_with("WAP"))
library(tidyverse)
library(lattice)
library(caret)
training_orig <- read.csv("C:/Users/Splendora/OneDrive/Ubiqum/Module_3_Task_3_EvaluateWiFi/trainingData.csv", header = TRUE)
validation_orig <- read.csv("C:/Users/Splendora/OneDrive/Ubiqum/Module_3_Task_3_EvaluateWiFi/validationData.csv", header = TRUE)
# check attributes
attributes(training_orig)
colnames(training_orig)[521] <- "Longitude"
colnames(training_orig)[522] <- "Latitude"
colnames(training_orig)[523] <- "Floor"
colnames(training_orig)[524] <- "Building_ID"
colnames(training_orig)[525] <- "Space_ID"
colnames(training_orig)[526] <- "Relative_Position"
colnames(training_orig)[527] <- "User_ID"
colnames(training_orig)[528] <- "Phone_ID"
colnames(training_orig)[529] <- "Timestamp"
library(tidyverse)
library(lattice)
library(caret)
training_orig <- read.csv("C:/Users/Splendora/OneDrive/Ubiqum/Module_3_Task_3_EvaluateWiFi/trainingData.csv", header = TRUE)
colnames(training_orig)[521] <- "Longitude"
colnames(training_orig)[522] <- "Latitude"
colnames(training_orig)[523] <- "Floor"
colnames(training_orig)[524] <- "Building_ID"
colnames(training_orig)[525] <- "Space_ID"
colnames(training_orig)[526] <- "Relative_Position"
colnames(training_orig)[527] <- "User_ID"
colnames(training_orig)[528] <- "Phone_ID"
colnames(training_orig)[529] <- "Timestamp"
test_orig <- round(nrow(training_orig)*0.10)
library(tidyverse)
library(lattice)
library(caret)
training_orig <- read.csv("C:/Users/Splendora/OneDrive/Ubiqum/Module_3_Task_3_EvaluateWiFi/trainingData.csv", header = TRUE)
# Rename attribute columns
colnames(training_orig)[521] <- "Longitude"
colnames(training_orig)[522] <- "Latitude"
colnames(training_orig)[523] <- "Floor"
colnames(training_orig)[524] <- "Building_ID"
colnames(training_orig)[525] <- "Space_ID"
colnames(training_orig)[526] <- "Relative_Position"
colnames(training_orig)[527] <- "User_ID"
colnames(training_orig)[528] <- "Phone_ID"
colnames(training_orig)[529] <- "Timestamp"
set.seed(745)
sample_from_original <- round(nrow(training_orig)*0.20)
train_sample <- round(nrow(sample_from_original)*0.70) #70% of sample set to be used for training
test_sample <- nrow(sample_from_original) - train_sample #30% of sample set to be used for testing
training_indices <- sample(seq_len(nrow(sample_from_original)), size = train_sample)
trainSet_sample <- sample_from_original[training_indices,]
testSet_sample <- sample_from_original[-training_indices,]
library(tidyverse)
library(lattice)
library(caret)
# import training data ====
training_orig <- read.csv("C:/Users/Splendora/OneDrive/Ubiqum/Module_3_Task_3_EvaluateWiFi/trainingData.csv", header = TRUE)
# validation_orig <- read.csv("C:/Users/Splendora/OneDrive/Ubiqum/Module_3_Task_3_EvaluateWiFi/validationData.csv", header = TRUE)
# Rename attribute columns
colnames(training_orig)[521] <- "Longitude"
colnames(training_orig)[522] <- "Latitude"
colnames(training_orig)[523] <- "Floor"
colnames(training_orig)[524] <- "Building_ID"
colnames(training_orig)[525] <- "Space_ID"
colnames(training_orig)[526] <- "Relative_Position"
colnames(training_orig)[527] <- "User_ID"
colnames(training_orig)[528] <- "Phone_ID"
colnames(training_orig)[529] <- "Timestamp"
# Create 10% Sample Set from training_orig data and then split to train and test sets
set.seed(745)
sample_from_original <- round(nrow(training_orig)*0.20) #20% sample from original data set
train_sample <- round(nrow(sample_from_original)*0.70) #70% of sample set to be used for training
test_sample <- nrow(sample_from_original) - train_sample #30% of sample set to be used for testing
training_indices <- sample(seq_len(nrow(sample_from_original)), size = train_sample)
sample_from_original
library(tidyverse)
library(lattice)
library(caret)
# import training data ====
training_orig <- read.csv("C:/Users/Splendora/OneDrive/Ubiqum/Module_3_Task_3_EvaluateWiFi/trainingData.csv", header = TRUE)
# validation_orig <- read.csv("C:/Users/Splendora/OneDrive/Ubiqum/Module_3_Task_3_EvaluateWiFi/validationData.csv", header = TRUE)
# Rename attribute columns
colnames(training_orig)[521] <- "Longitude"
colnames(training_orig)[522] <- "Latitude"
colnames(training_orig)[523] <- "Floor"
colnames(training_orig)[524] <- "Building_ID"
colnames(training_orig)[525] <- "Space_ID"
colnames(training_orig)[526] <- "Relative_Position"
colnames(training_orig)[527] <- "User_ID"
colnames(training_orig)[528] <- "Phone_ID"
colnames(training_orig)[529] <- "Timestamp"
# Create 10% Sample Set from training_orig data and then split to train and test sets
set.seed(745)
sample_from_original <- round(nrow(training_orig)*0.20) #20% sample from original data set
inTraining <- createDataPartition(sample_from_original, p = .75, list = FALSE)
inTraining <- createDataPartition(sample_from_original$Building, p = .75, list = FALSE)
inTraining <- createDataPartition(sample_from_original$Building_ID, p = .75, list = FALSE)
inTraining <- createDataPartition(sample_from_original$Building_ID, p = .75, list = FALSE)
# # summarize data
# summary(training_orig$Latitude)
#
# # check data type
# str(training_orig)
#
# # check for missing values
# sum(is.na(training_orig))
#
# Visualize min/max values of non-RSSI attributes
nonRSSI_orig <- training_orig %>%
select(Longitude, Latitude, Floor, Building_ID, Space_ID, Relative_Position, User_ID, Phone_ID, Timestamp)
str(nonRSSI_orig)
library(tidyverse)
library(lattice)
library(caret)
# import training data ====
training_orig <- read.csv("C:/Users/Splendora/OneDrive/Ubiqum/Module_3_Task_3_EvaluateWiFi/trainingData.csv", header = TRUE)
# validation_orig <- read.csv("C:/Users/Splendora/OneDrive/Ubiqum/Module_3_Task_3_EvaluateWiFi/validationData.csv", header = TRUE)
# Rename attribute columns
colnames(training_orig)[521] <- "Longitude"
colnames(training_orig)[522] <- "Latitude"
colnames(training_orig)[523] <- "Floor"
colnames(training_orig)[524] <- "Building_ID"
colnames(training_orig)[525] <- "Space_ID"
colnames(training_orig)[526] <- "Relative_Position"
colnames(training_orig)[527] <- "User_ID"
colnames(training_orig)[528] <- "Phone_ID"
colnames(training_orig)[529] <- "Timestamp"
train_sample <- sample.split(training_orig, SplitRatio = .70) #20% sample from original data set
library(caTools)
train_sample <- sample.split(training_orig, SplitRatio = .70) #20% sample from original data set
library(tidyverse)
library(lattice)
library(caret)
library(caTools)
training_orig <- read.csv("C:/Users/Splendora/OneDrive/Ubiqum/Module_3_Task_3_EvaluateWiFi/trainingData.csv", header = TRUE)
# Rename attribute columns
colnames(training_orig)[521] <- "Longitude"
colnames(training_orig)[522] <- "Latitude"
colnames(training_orig)[523] <- "Floor"
colnames(training_orig)[524] <- "Building_ID"
colnames(training_orig)[525] <- "Space_ID"
colnames(training_orig)[526] <- "Relative_Position"
colnames(training_orig)[527] <- "User_ID"
colnames(training_orig)[528] <- "Phone_ID"
colnames(training_orig)[529] <- "Timestamp"
# Create 20% Sample Set from training_orig data and then split to train and test sets
set.seed(745)
trainSize_sample <- round(nrow(training_orig)*0.20)
testSize_sample <- nrow(training_orig) - trainSize_sample
training_indices <- sample(seq_len(nrow(training_orig)), size = trainSize_sample)
library(tidyverse)
library(lattice)
library(caret)
training_orig <- read.csv("C:/Users/Splendora/OneDrive/Ubiqum/Module_3_Task_3_EvaluateWiFi/trainingData.csv", header = TRUE)
colnames(training_orig)[521] <- "Longitude"
colnames(training_orig)[522] <- "Latitude"
colnames(training_orig)[523] <- "Floor"
colnames(training_orig)[524] <- "Building_ID"
colnames(training_orig)[525] <- "Space_ID"
colnames(training_orig)[526] <- "Relative_Position"
colnames(training_orig)[527] <- "User_ID"
colnames(training_orig)[528] <- "Phone_ID"
colnames(training_orig)[529] <- "Timestamp"
# Create 20% Sample Set from training_orig data and then split to train and test sets
set.seed(745)
trainSize_sample <- round(nrow(training_orig)*0.20)
testSize_orig <- nrow(training_orig) - trainSize_sample
training_indices <- sample(seq_len(nrow(training_orig)), size = trainSize_sample)
trainSet_sample <- training_orig[training_indices,]
testSet_sample <- training_orig[-training_indices,]
# applying 10-fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 2)
nonRSSI_orig <- training_orig %>%
select(Longitude, Latitude, Floor, Building_ID, Space_ID, Relative_Position, User_ID, Phone_ID, Timestamp)
# Separate data by building from sample_set
sample_building0 <- filter(trainSet_sample, Building_ID == 0)
sample_building1 <- filter(trainSet_sample, Building_ID == 1)
sample_building2 <- filter(trainSet_sample, Building_ID == 2)
sample_building3 <- filter(trainSet_sample, Building_ID == 3)
# Create a data frame for each feature from sample_set
## Building0
sample_building0_floor <- data.frame(sample_building0$Floor,
sample_building0 %>%
select(starts_with("WAP")))
sample_building0_latitude <- data.frame(sample_building0$Latitude,
sample_building0 %>%
select(starts_with("WAP")))
sample_building0_longitude <- data.frame(sample_building0$Longitude,
sample_building0 %>%
select(starts_with("WAP")))
## Building1
sample_building1_floor <- data.frame(sample_building1$Floor,
sample_building1 %>%
select(starts_with("WAP")))
sample_building1_latitude <- data.frame(sample_building1$Latitude,
sample_building1 %>%
select(starts_with("WAP")))
sample_building1_longitude <- data.frame(sample_building1$Longitude,
sample_building1 %>%
select(starts_with("WAP")))
## Building2
sample_building2_floor <- data.frame(sample_building2$Floor,
sample_building2 %>%
select(starts_with("WAP")))
sample_building2_latitude <- data.frame(sample_building2$Latitude,
sample_building2 %>%
select(starts_with("WAP")))
sample_building2_longitude <- data.frame(sample_building2$Longitude,
sample_building2 %>%
select(starts_with("WAP")))
## Building3
sample_building3_floor <- data.frame(sample_building3$Floor,
sample_building3 %>%
select(starts_with("WAP")))
sample_building3_latitude <- data.frame(sample_building3$Latitude,
sample_building3 %>%
select(starts_with("WAP")))
sample_building3_longitude <- data.frame(sample_building3$Longitude,
sample_building3 %>%
select(starts_with("WAP")))
knnTrain_building0 <- train(Floor~.,
data = sample_building0,
method = "knn",
trControl = fitControl,
tuneLength = 2,
preProcess=c("center","scale"))
knnTrain_building0
varImp(knnTrain_building0)
knnPredict_building0 <- predict(knnTrain_building0, testSet_sample)
knnPredict_building0
## Confusion Matrix
confusionMatrix(knnPredict_building0,(testSet_sample))
## Confusion Matrix
confusionMatrix(knnPredict_building0,(testSet_sample$Building_ID))
varImp(knnTrain_building0)
summary(training_orig$WAP001)
summary(training_orig$WAP002)
library(tidyverse)
library(lattice)
library(caret)
# import training data ====
training_orig <- read.csv("C:/Users/Splendora/OneDrive/Ubiqum/Module_3_Task_3_EvaluateWiFi/trainingData.csv", header = TRUE)
# validation_orig <- read.csv("C:/Users/Splendora/OneDrive/Ubiqum/Module_3_Task_3_EvaluateWiFi/validationData.csv", header = TRUE)
# Rename attribute columns
colnames(training_orig)[521] <- "Longitude"
colnames(training_orig)[522] <- "Latitude"
colnames(training_orig)[523] <- "Floor"
colnames(training_orig)[524] <- "Building_ID"
colnames(training_orig)[525] <- "Space_ID"
colnames(training_orig)[526] <- "Relative_Position"
colnames(training_orig)[527] <- "User_ID"
colnames(training_orig)[528] <- "Phone_ID"
colnames(training_orig)[529] <- "Timestamp"
# Visualize non-RSSI attributes
nonRSSI_orig <- training_orig %>%
select(Longitude, Latitude, Floor, Building_ID, Space_ID, Relative_Position, User_ID, Phone_ID, Timestamp)
# Create 20% Sample Set from training_orig data and then split to train and test sets
set.seed(745)
trainSize_sample <- round(nrow(training_orig)*0.20)
testSize_orig <- nrow(training_orig) - trainSize_sample
training_indices <- sample(seq_len(nrow(training_orig)), size = trainSize_sample)
trainSet_sample <- training_orig[training_indices,]
testSet_sample <- training_orig[-training_indices,]
sample_building0 <- filter(trainSet_sample, Building_ID == 0)
sample_building1 <- filter(trainSet_sample, Building_ID == 1)
sample_building2 <- filter(trainSet_sample, Building_ID == 2)
sample_building3 <- filter(trainSet_sample, Building_ID == 3)
sample_building0_floor <- data.frame(sample_building0$Floor,
sample_building0 %>%
select(starts_with("WAP")))
sample_building0_latitude <- data.frame(sample_building0$Latitude,
sample_building0 %>%
select(starts_with("WAP")))
sample_building0_longitude <- data.frame(sample_building0$Longitude,
sample_building0 %>%
select(starts_with("WAP")))
## Building1
sample_building1_floor <- data.frame(sample_building1$Floor,
sample_building1 %>%
select(starts_with("WAP")))
sample_building1_latitude <- data.frame(sample_building1$Latitude,
sample_building1 %>%
select(starts_with("WAP")))
sample_building1_longitude <- data.frame(sample_building1$Longitude,
sample_building1 %>%
select(starts_with("WAP")))
## Building2
sample_building2_floor <- data.frame(sample_building2$Floor,
sample_building2 %>%
select(starts_with("WAP")))
sample_building2_latitude <- data.frame(sample_building2$Latitude,
sample_building2 %>%
select(starts_with("WAP")))
sample_building2_longitude <- data.frame(sample_building2$Longitude,
sample_building2 %>%
select(starts_with("WAP")))
## Building3
sample_building3_floor <- data.frame(sample_building3$Floor,
sample_building3 %>%
select(starts_with("WAP")))
sample_building3_latitude <- data.frame(sample_building3$Latitude,
sample_building3 %>%
select(starts_with("WAP")))
sample_building3_longitude <- data.frame(sample_building3$Longitude,
sample_building3 %>%
select(starts_with("WAP")))
## applying 10-fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 2)
knnTrain_building0 <- train(Floor~.,
data = sample_building0,
method = "knn",
trControl = fitControl,
tuneLength = 2,
preProcess=c("center","scale"))
## result of training
knnTrain_building0
## variable importance
varImp(knnTrain_building0)
## Predicting the test set
knnPredict_building0 <- predict(knnTrain_building0, testSet_sample)
knnPredict_building0
## Confusion Matrix
confusionMatrix(knnPredict_building0,(testSet_sample$Building_ID))
## Confusion Matrix
confusionMatrix(knnPredict_building0,testSet_sample$Building_ID)
## Confusion Matrix
confusionMatrix(table(knnPredict_building0,testSet_sample$Building_ID))
str(knnPredict_building0)
str(testSet_sample$Building_ID)
testSet_sample$Building_ID <- as.numeric(testSet_sample$Building_ID)
str(knnPredict_building0)
str(testSet_sample$Building_ID)
## Confusion Matrix
confusionMatrix(knnPredict_building0,testSet_sample$Building_ID)
testSet_sample$Building_ID <- as.integer(testSet_sample$Building_ID)
## Confusion Matrix
confusionMatrix(knnPredict_building0,testSet_sample$Building_ID)
## Confusion Matrix
confusionMatrix(knnPredict_building0,testSet_sample$Floor)
str(knnPredict_building0)
str(testSet_sample$Floor)
# Create a data frame for each feature from sample_set
## Building0
sample_building0_floor <- data.frame(as.factor(sample_building0$Floor,
sample_building0 %>%
select(starts_with("WAP"))))
# Create a data frame for each feature from sample_set
## Building0
sample_building0_floor <- data.frame(as.factor(sample_building0$Floor,
sample_building0 %>%
select(starts_with("WAP"))))
sample_building0
# Separate data by building from sample_set
sample_building0 <- filter(trainSet_sample, Building_ID == 0)
str(sample_building0)
sample_building0 <- as.factor(sample_building0)
## Confusion Matrix
confusionMatrix(knnPredict_building0,testSet_sample$Building_ID)
set.seed(745)
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 2)
knnTrain_Floor <- train(Floor~.,
data = sample_building0,
method = "knn",
trControl = fitControl,
tuneLength = 2,
preProcess=c("center","scale"))
knnPredict_building0 <- predict(knnTrain_building0, testSet_sample)
knnPredict_building0
knnTrain_Floor <- train(Floor~.,
data = sample_building0,
method = "knn",
trControl = fitControl,
tuneLength = 2,
preProcess=c("center","scale"))
knnTrain_Floor <- train(Floor~.,
data = sample_building0,
method = "knn",
trControl = fitControl,
tuneLength = 2,
preProcess=c("center","scale"))
sample_building0
View(sample_building0_floor)
str(sample_building0_floor)
View(trainSet_sample)
